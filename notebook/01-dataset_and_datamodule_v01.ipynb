{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and DataModule V01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
    "        max_seq_len: int = 512,\n",
    "        phase: str = \"train\",\n",
    "    ):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        document, summary = item[\"total\"], item[\"summary\"]  # TODO: 데이터에 맞게끔 변경\n",
    "\n",
    "        encoder_input_id, encoder_attention_mask = self.encode_and_pad(document)\n",
    "\n",
    "        if self.phase in [\"train\", \"valid\"]:\n",
    "            decoder_input_id, decoder_attention_mask = self.encode_and_pad(summary)\n",
    "\n",
    "            output_id = self.tokenizer.encode(summary)\n",
    "            output_id += [self.tokenizer.eos_token_id]\n",
    "            if len(output_id) < self.max_seq_len:\n",
    "                pad_len = self.max_seq_len - len(output_id)\n",
    "                output_id += [-999999] * pad_len\n",
    "            else:\n",
    "                output_id = output_id[: self.max_seq_len - 1] + [\n",
    "                    self.tokenizer.eos_token_id\n",
    "                ]\n",
    "            return {\n",
    "                \"input_ids\": np.array(encoder_input_id, dtype=np.int_),\n",
    "                \"attention_mask\": np.array(encoder_attention_mask, dtype=np.float32),\n",
    "                \"decoder_input_ids\": np.array(decoder_input_id, dtype=np.int_),\n",
    "                \"decoder_attention_mask\": np.array(\n",
    "                    decoder_attention_mask, dtype=np.float32\n",
    "                ),\n",
    "                \"labels\": np.array(output_id, dtype=np.int_),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": np.array(encoder_input_id, dtype=np.int_),\n",
    "                \"attention_mask\": np.array(encoder_attention_mask, dtype=np.float32),\n",
    "            }\n",
    "\n",
    "    def encode_and_pad(self, text: str):\n",
    "        # token_to_id\n",
    "        # encoder_input_id = self.tokenizer.encode(document)\n",
    "        tokens = (\n",
    "            [self.tokenizer.bos_token]\n",
    "            + self.tokenizer.tokenize(text)\n",
    "            + [self.tokenizer.eos_token]\n",
    "        )\n",
    "        input_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "\n",
    "        # padding\n",
    "        if len(input_id) < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - len(input_id)\n",
    "            input_id += [self.tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "        else:\n",
    "            input_id = input_id[: self.max_seq_len - 1] + [self.tokenizer.eos_token_id]\n",
    "            attention_mask = attention_mask[: self.max_seq_len]\n",
    "        return input_id, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "train_path = \"../data/train.csv\"\n",
    "tokenizer_path = \"../kobart\"\n",
    "max_seq_len = 512\n",
    "phase = \"train\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "tokenizer = get_kobart_tokenizer(tokenizer_path)\n",
    "\n",
    "train_dataset = SummaryDataset(\n",
    "    data=train,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=max_seq_len,\n",
    "    phase=phase,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[100]\n",
    "# sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int,\n",
    "        valid_size: float = 0.2,\n",
    "        batch_size: int = 8,\n",
    "        num_workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.valid_size = valid_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # load data & tokenizer\n",
    "        train = pd.read_csv(self.train_path)\n",
    "        test = pd.read_csv(self.test_path)\n",
    "        tokenizer = get_kobart_tokenizer(self.tokenizer_path)\n",
    "\n",
    "        # split train/valid\n",
    "        train, valid = train_test_split(train, test_size=self.valid_size, shuffle=True)\n",
    "\n",
    "        # train/valid/test Dataset\n",
    "        self.trainset = SummaryDataset(\n",
    "            train, tokenizer, self.max_seq_len, phase=\"train\"\n",
    "        )\n",
    "        self.validset = SummaryDataset(\n",
    "            valid, tokenizer, self.max_seq_len, phase=\"valid\"\n",
    "        )\n",
    "        self.testset = SummaryDataset(valid, tokenizer, self.max_seq_len, phase=\"test\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testset, batch_size=self.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/train.csv\"\n",
    "test_path = \"../data/test.csv\"\n",
    "tokenizer_path = \"../kobart\"\n",
    "max_seq_len = 512\n",
    "valid_size = 0.2\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "\n",
    "data_module = SummaryDataModule(\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    valid_size=valid_size,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "valid_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca37adc465479a74c6a141ab03dbdf7641081253eef51a46dbf42999bc11eb1a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pt-py37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
